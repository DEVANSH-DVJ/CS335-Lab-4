\documentclass[12pt, fleqn]{article}

\usepackage[left=0.75in, right=0.75in, bottom=0.75in, top=1.0in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{floatrow}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{sectsty}
\usepackage{enumitem}
\renewcommand{\thesubsubsection}{(\roman{subsubsection})}

\usepackage[dvipsnames]{xcolor}
\usepackage[perpage]{footmisc}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{190100044}
\rhead{Assignment 4}
\renewcommand{\footrulewidth}{1.0pt}
\cfoot{Page \thepage}

\setlength{\parindent}{0em}

\title{Assignment 4}
\author{Devansh Jain, 190100044}
\date{30th Oct 2021}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

% \pagenumbering{gobble}
\maketitle
\tableofcontents
\thispagestyle{empty}
\setcounter{page}{0}

\newpage
\section{Clustering}
\subsection{CS 335 KMeans Implementation}



\newpage
\section{Kernel design and Kernelized clustering}
\subsection{CS 337: Proving Kernel Validity}
We are going to use the following property of kernels from Lecture slides (Lecture 11):
\begin{equation*}
    \begin{aligned}
        K(x,y) = \sum_{d=1}^r \alpha_d(x^T y)^d \text{ where } \alpha_d \ge 0 \text{ is a kernel } (r \text{ can be } \infty)
    \end{aligned}
\end{equation*}

Another property of kernels which we are going to use is:
\begin{equation*}
    \begin{aligned}
         & K(x, y) \text{ is a kernel } \implies K'(x, y) = f(x) f(y) K(x, y) \text{ is also a kernel} \\
         & \text{Corresponding feature map, } \phi'(x) := f(x) \phi(x)
    \end{aligned}
\end{equation*}


An important property of exponential function which we are going to exploit is:
\begin{equation*}
    \begin{aligned}
        \exp(x) = \sum_{k=0}^{\infty} \frac{x^k}{k!}
    \end{aligned}
\end{equation*}

Using the above properties, we can conclude that $\exp(\alpha x^T y)$ where $\alpha \ge 0$ is a kernel.

Now, if take $\alpha = \dfrac{1}{\sigma^2}$ and $f(x) = \exp\bigg(- \dfrac{x^T x}{2 \sigma^2}\bigg)$, we get: \\
$K_\alpha (x, y) = \exp\bigg(- \dfrac{||x - y||^2}{2 \sigma^2}\bigg) = \exp\bigg(- \dfrac{x^T x}{2 \sigma^2}\bigg) \exp\bigg(- \dfrac{y^T y}{2 \sigma^2}\bigg) \exp\bigg(\dfrac{x^T y}{\sigma^2}\bigg)$ is a kernel.

Hence, proved. \hfill $\qed$


\subsection{CS 337: Simple Kernel Design}
\subsubsection{}
We proved in Lecture slides (Lecture 13, Part 2) that the vanilla KMeans algorithm would converge in finite number of iterations (say, $N$). \\
Without loss of generality, let $r_1 \ge r_2$. \\
For the sake of contradiction, let's assume that after $N$ iterations we get ${\mu_1, \mu_2}$ which can correctly classify the clusters. Obviously, $\mu_1 \neq \mu_2$. \\
So, blue points have $Pr_1 = 1$ and $Pr_2 = 0$ while red points have $Pr_1 = 0$ and $Pr_2 = 1$. \\
As the algorithm has converged, the $Pr$s for all points and $\mu$s won't change any further. \\

The decision boundary would be $f(x) = \bigg(x - \dfrac{\mu_1 + \mu_2}{2}\bigg) \cdot (\mu_2 - \mu_1) = (x \cdot (\mu_2 - \mu_1)) - \dfrac{||\mu_1 - \mu_2||^2}{2}$. \\
This decision boundary is linear in $x$, therefore, due to radial symmetry some of blue and red points would be on same side. \\
However, we know that $Pr_1 = 0$ for red points and $Pr_1 = 1$ for blue points. So in $(N+1)^{th}$ iteration, the values of $Pr$s would change for some points. \\

This leads to contradiction. \\
Thus, even after convergence, the vanilla KMeans algorithms won't be able to correctly classify the clusters for any value of $r_1$ and $r_2$.

\subsubsection{}



\end{document}
